# LLM

::: details 参考资料：

- [OpenAI](https://openai.com/)
- [DeepSeek](https://www.deepseek.com/)

:::

「LLM」（Large Language Model，大语言模型），指基于海量文本数据训练、能够理解和生成自然语言的人工智能模型。

## 基础知识

🧠 LLM 就像是一个超级语言大脑，它读了全世界的大量书籍和网页，可以和你“交流”。

### 1.什么是LLM？

简单点说：

👉 LLM 就是一个能读、理解、写作、翻译甚至对话的智能系统。

LLM 通常基于 `Transformer` 架构（如GPT、PaLM、LLaMA、DeepSeek等），通过深度学习技术从数据中学习语言规律，并具备强大的文本生成、推理和上下文理解能力。

### 2.训练和使用

✅ 它是怎么训练的？

LLM 是在海量的文本数据上训练的，比如：

- 书籍
- 网站内容
- 新闻文章
- 社交媒体文本

通过这些数据，它学会了语言的结构、用法、逻辑关系和常识。

✅ 它能做什么？

- 回答问题
- 翻译语言
- 总结文章
- 写故事、诗歌、代码
- 自动回复邮件、生成文案
- 写论文、做研究辅助

✅ 为什么它“大”？

因为模型的参数量非常巨大。

像 GPT-4 这样的模型，参数量可能达到千亿级别。参数越多，它理解和生成语言的能力就越强。

## 向量化

「向量化」这个词是 RAG 和 LLM 系统里非常关键的概念，是指`将一句话/一段文字变成数字数组`。

### 1.为什么需要向量化？

计算机不懂文字，只能处理数字。想让机器`理解文本的意思`，就得先把文本变成一个数字的形式，那就是：`向量`（vector）

📦 举个例子：

原文是：

> "服务A在14点宕机，日志中出现OOM错误"

向量化后可能变成一个高维数组：

> [0.024, -0.335, 0.910, ..., 0.103]  （比如有 768 维）

这个向量就代表了这段文字的语义，而且**相似含义的句子，它们的向量也会很接近**！

比如下面这两句话：

- “服务A崩溃，内存超了”
- “服务A在14点宕机，日志中出现OOM错误”

虽然字不一样，但意思很接近，它们的向量在空间中也会靠得很近。

### 2.向量的作用是什么？

让我们能用数学方式比较语义。比如：

👇用户提问：
“为什么服务A在下午挂了？”

我们可以把这个问题向量化，然后在我们的知识库里找出：

“哪个文本块的向量和它最近？” ➜ 那些文本块就很可能包含了答案！

### 3.向量是怎么生成的？

使用 `Embedding模型` 来做，常见的模型有：

| 模型名称                            | 	特点            |
|---------------------------------|----------------|
| `OpenAI text-embedding-3-small` | 准确、支持多语言（收费）   |
| `BGE (bge-large-zh)`	           | 中文超强、开源免费      |
| `E5`	                           | 多语言好，开源，适合英文场景 |
| `text2vec`	                     | 中文轻量级模型，适合本地部署 |

## prompt

Prompt 是让 LLM 知道“你要干嘛”的输入语句，填充 Prompt 就是动态插入内容，让模型更精准地理解任务背景。

### 1.prompt

一句话解释：

- prompt = 给大语言模型的一段`提示词`或`输入文本`，告诉它你想让它干什么
- 填充 prompt = 把你准备好的内容（比如检索结果、用户输入）动态插入到 prompt 模板

举个最简单的例子：

- 比如你要问 ChatGPT： “用一句话解释什么是 RAG？”
- 这个提问，其实就是一个 prompt

更通用地说： Prompt 就像是你给 LLM 下的一段命令，告诉它你希望它如何回应。

### 2.填充prompt

比如一个稍复杂的 prompt 可能是：

> 你是一个经验丰富的运维工程师。
> 
> 请根据以下日志判断错误原因，并提供可能的处理建议：
> 
> 日志内容如下： 
> 
> {{log_content}}

其中的 {{log_content}} 就是需要填充的部分。

所谓`填充 Prompt`就是在真正调用 LLM 之前，**把 prompt 模板里需要替换的变量**，换成你当前要处理的数据。

比如用户上传了这个日志：

> 服务A 在14点发生OOM错误，PID 12345终止，重启失败。

你在代码里就会做：

> prompt = prompt_template.replace("{{log_content}}", user_uploaded_log)

生成后的实际 Prompt 是：

> 你是一个经验丰富的运维工程师。请根据以下日志判断错误原因，并提供可能的处理建议： 
> 
> 日志内容如下：
> 
> 服务A 在14点发生OOM错误，PID 12345终止，重启失败。

然后再把这个 Prompt 发送给大模型，得到结果。

## 工作流

### 1.工作流是什么？

「工作流」（`Workflow`）是啥，工作流就是一个任务执行流程图，每个节点完成一个具体的动作（如 LLM 生成、调用插件、检索知识库等），所有节点之间按顺序流转，最终完成一项智能任务。

可以把它理解为：
- 一个“搭积木”式的 AI 自动流程，用来让 LLM 能根据用户输入，有逻辑地、自动地做事情

### 2.工作流作用

为什么要有“工作流”？

因为实际应用中，一个 LLM 调用远远不够，你经常要：

- 判断输入
- 查询数据库/知识库
- 插入业务流程（审批、工单）
- 多轮交互
- 外部系统对接

这些逻辑串起来，靠工作流来管理最合适。

### 3.Dify

在 Dify 中，工作流是一种基于节点的图形化流程控制器，它允许你将 LLM 调用、知识库检索、外部插件和条件判断等能力以流程方式组织起来，从而构建出复杂的智能应用。

每个节点独立完成一个动作，所有节点通过上下文和连线串联执行。

以 Dify 为例，工作流包含组件为：

1. 节点（Node）

每个节点就是一个动作，例如：

| 节点类型	     | 功能举例                 |
|-----------|----------------------|
| LLM 节点    | 	调用大模型生成回答           |
| 知识库节点	    | 查询特定知识库（RAG）         |
| 插件节点	     | 调用 API，比如查询数据库、重启服务等 |
| 条件节点      | 	根据意图/关键词，决定后续走哪条分支  |
| ️ 用户输入节点	 | 从用户收集输入（对话、参数等）      |

2. 连线（Edge）

表示任务执行顺序，节点之间通过连线相连，数据会一环一环往下传

3. 上下文（Context）

整个工作流在执行时共享一份上下文变量（像一个字典），每个节点可以读写它，最终可以用上下文变量里的数据来填充 prompt 或做判断

举个实际例子：

- Dify 里一个智能运维的工作流，假设我们做一个“日志异常分析助手”，流程如下：

````text
[用户输入：上传一段错误日志]
         ↓
[LLM 节点1：抽取异常关键词]
         ↓
[知识库节点：用关键词去查公司运维文档]
         ↓
[LLM 节点2：总结问题成自然语言并给处理建议]
         ↓
[插件节点：是否调用运维 API 自动执行？]
         ↓
[返回用户最终答案]
````

